# ğŸ“° AnÃ¡lisis Completo del Proyecto "Portal de Noticias con Scraping e IA"

**Fecha de AnÃ¡lisis:** 26 de noviembre de 2025  
**Ãšltima ActualizaciÃ³n del README:** Desactualizado (ver recomendaciones al final)

---

## ğŸ¯ PropÃ³sito General del Proyecto

Es una **plataforma web de agregaciÃ³n de noticias** que:
1. **Scrapea noticias** automÃ¡ticamente de dos portales peruanos: El Comercio y PerÃº21
2. **Almacena** las noticias en una BD MySQL con control de usuarios
3. **Analiza** noticias con IA (OpenRouter) para extraer resumen, sentimiento, categorÃ­a y entidades
4. **Gestiona usuarios** con roles (normal, premium, admin) y seguimiento de actividades
5. **Ejecuta tareas asincrÃ³nicas** via Celery + Redis para scraping y anÃ¡lisis sin bloquear

---

## ğŸ“± Estructura del Proyecto

```
web_scraping/                    # Proyecto Django principal
â”œâ”€â”€ settings.py                  # ConfiguraciÃ³n (Django, BD, Celery, OpenRouter)
â”œâ”€â”€ celery.py                    # ConfiguraciÃ³n de Celery
â”œâ”€â”€ urls.py                      # URLs globales
â”œâ”€â”€ wsgi.py / asgi.py           # Interfaces de servidor

accounts/                        # GestiÃ³n de usuarios y perfiles
â”œâ”€â”€ models.py                   # Profile (roles), Actividad
â”œâ”€â”€ views.py                    # Auth (login, register, profile, premium)
â”œâ”€â”€ forms.py                    # RegistroForm
â”œâ”€â”€ admin.py                    # Admin personalizado
â””â”€â”€ urls.py                     # Rutas de usuarios

scraping/                        # Web scraping de noticias
â”œâ”€â”€ models.py                   # Noticia, NoticiasVistas
â”œâ”€â”€ views.py                    # Listados de noticias por secciÃ³n
â”œâ”€â”€ tasks.py                    # scrape_all_sections, run_single_scrape (Celery)
â”œâ”€â”€ urls.py                     # Rutas de scraping y noticias
â””â”€â”€ management/commands/        # Django commands para scrapers
    â”œâ”€â”€ scrape_elcomercio.py    # Scrapea El Comercio (Playwright)
    â”œâ”€â”€ scrape_economia.py      # SecciÃ³n EconomÃ­a de El Comercio
    â”œâ”€â”€ scrape_elcomercio_pol.py # SecciÃ³n PolÃ­tica de El Comercio
    â”œâ”€â”€ scrape_mundo.py         # SecciÃ³n Mundo de El Comercio
    â”œâ”€â”€ scrape_tecnologia.py    # SecciÃ³n TecnologÃ­a de El Comercio
    â”œâ”€â”€ scrape_peru21.py        # Scrapea PerÃº21 (Playwright)
    â”œâ”€â”€ scrape_peru21D.py       # SecciÃ³n Deportes de PerÃº21
    â”œâ”€â”€ scrape_peru21G.py       # SecciÃ³n GastronomÃ­a de PerÃº21
    â”œâ”€â”€ scrape_peru21I.py       # SecciÃ³n InvestigaciÃ³n de PerÃº21
    â”œâ”€â”€ scrape_peru21L.py       # SecciÃ³n Lima de PerÃº21
    â””â”€â”€ cleaned_beat.py         # Arranca Beat limpiando schedule

analisis/                        # AnÃ¡lisis de noticias con IA
â”œâ”€â”€ models.py                   # AnalisisNoticia (resumen, sentimiento, etc.)
â”œâ”€â”€ views.py                    # API endpoints para iniciar/consultar anÃ¡lisis
â”œâ”€â”€ tasks.py                    # analizar_noticia_task (Celery)
â”œâ”€â”€ urls.py                     # Rutas de anÃ¡lisis
â””â”€â”€ admin.py                    # Admin de anÃ¡lisis

templates/                       # Plantillas HTML (Bootstrap, JS interactivo)
â”œâ”€â”€ base.html                   # Base principal
â”œâ”€â”€ accounts/                   # Login, registro, perfil, planes
â”œâ”€â”€ noticias/                   # Listados de El Comercio por secciÃ³n
â”œâ”€â”€ peru21/                     # Listados de PerÃº21 por secciÃ³n
â””â”€â”€ analisis/                   # Resultados de anÃ¡lisis

static/                         # CSS, JS
â””â”€â”€ [css, js]/

requirements.txt               # Dependencias
```

---

## ğŸ”§ ConfiguraciÃ³n TÃ©cnica

### **Stack TecnolÃ³gico**

| Componente | TecnologÃ­a | VersiÃ³n |
|-----------|-----------|---------|
| **Framework Web** | Django | 5.2.6 |
| **Motor de Colas** | Celery | 5.5.3 |
| **Broker de Mensajes** | Redis | Configurado en settings |
| **Scraping** | Playwright | 1.55.0 |
| **Parsing HTML** | BeautifulSoup4 | 4.13.5 |
| **IA (anÃ¡lisis)** | OpenAI (via OpenRouter) | 2.8.1 |
| **BD** | MySQL | Via `mysqlclient==2.2.7` |
| **AutenticaciÃ³n JWT** | PyJWT | 2.10.1 |
| **ValidaciÃ³n de datos** | Pydantic | 2.12.4 |
| **HTTP** | httpx, requests | 0.28.1, 2.32.5 |
| **Parsing de datos** | python-dateutil | 2.9.0.post0 |

### **Base de Datos**

```python
# settings.py - ConfiguraciÃ³n MySQL
DATABASES = {
    'default': {
        'ENGINE': os.getenv('DB_ENGINE', 'django.db.backends.mysql'),
        'NAME': os.getenv('DB_NAME', 'elcomercio_db'),
        'USER': os.getenv('DB_USER', 'root'),
        'PASSWORD': os.getenv('DB_PASSWORD', ''),
        'HOST': os.getenv('DB_HOST', 'localhost'),
        'PORT': os.getenv('DB_PORT', '3306'),
        'OPTIONS': {
            'sql_mode': 'traditional',
            'charset': 'utf8mb4',
            'use_unicode': True,
        },
    }
}
```

### **ConfiguraciÃ³n Celery**

```python
# settings.py
CELERY_BROKER_URL = 'redis://localhost:6379/0'        # Redis como broker
CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'    # Almacenar resultados en Redis
CELERY_TIMEZONE = 'America/Lima'
CELERY_BEAT_SCHEDULE = {
    'scrape-all-every-5-hours': {
        'task': 'scraping.tasks.scrape_all_sections',
        'schedule': 5 * 60 * 60.0,  # 5 horas
    },
}
```

**Comandos para ejecutar Celery:**
```bash
# Worker (ejecuta las tareas)
celery -A web_scraping worker --loglevel=info --pool=solo

# Beat (ejecuta tareas programadas)
python manage.py cleaned_beat  # Con limpieza automÃ¡tica de schedule
```

### **ConfiguraciÃ³n OpenRouter (IA)**

```python
# settings.py - Se carga desde .env
OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY', '')
OPENROUTER_MODEL = os.getenv('OPENROUTER_MODEL', 'openai/gpt-3.5-turbo')
OPENROUTER_BASE_URL = 'https://openrouter.ai/api/v1'
```

---

## ğŸ“š DescripciÃ³n Detallada de Apps

### **1ï¸âƒ£ App `accounts` - GestiÃ³n de Usuarios**

**PropÃ³sito:** AutenticaciÃ³n, perfiles de usuario, roles, historial de actividades.

#### **Modelos:**
- **`Profile`** 
  - OneToOneField a User (Django built-in)
  - Roles: `normal`, `premium`, `admin`
  - Campo `fecha_registro`: CuÃ¡ndo se registrÃ³ el usuario
  - MÃ©todos Ãºtiles:
    - `dias_activo()`: Calcula dÃ­as desde el registro
    - `es_usuario_nuevo()`: True si tiene < 7 dÃ­as

- **`Actividad`**
  - Registra TODAS las acciones del usuario
  - Tipos: `vista`, `busqueda`, `compartir`, `login`, `scraping`
  - Almacena FK a Noticia y datos extras (JSON)
  - Ordenado por `-fecha_actividad`

#### **Vistas principales:**
- `register()`: Registro pÃºblico (rol siempre `normal`)
- `user_login()`: Login con registro de actividad
- `profile()`: Panel de usuario con stats (noticias vistas, anÃ¡lisis, actividades)
- `planes()`: Muestra planes disponibles
- `upgrade_premium()`: Cambiar rol a `premium`
- `cancelar_premium()`: Volver a rol `normal`

#### **Admin personalizado:**
- Interfaz mejorada para crear usuarios con email obligatorio
- Inline de Profile para editar rol desde User
- Lista filtrable de usuarios por rol

---

### **2ï¸âƒ£ App `scraping` - Web Scraping de Noticias**

**PropÃ³sito:** Extraer noticias de El Comercio y PerÃº21, almacenarlas en BD.

#### **Modelos:**
- **`Noticia`**
  - Campos: `titulo`, `autor`, `fecha`, `imagen` (URL), `enlace` (URL), `origen` (elcomercio/peru21), `fecha_scraping`
  - Ordenado por `-fecha, -fecha_scraping`
  - Clave Ãºnica: `titulo + origen` (no duplica noticias)

- **`NoticiasVistas`**
  - Tracking: QuÃ© usuario vio quÃ© noticia
  - unique_together: `(usuario, noticia)`
  - Clave para mostrar "noticias vistas" en el perfil

#### **Scrapers (Management Commands):**

**El Comercio (6 comandos):**
1. `scrape_elcomercio` - Todas las secciones desde portada
2. `scrape_economia` - SecciÃ³n EconomÃ­a
3. `scrape_elcomercio_pol` - SecciÃ³n PolÃ­tica
4. `scrape_mundo` - SecciÃ³n Mundo
5. `scrape_tecnologia` - SecciÃ³n TecnologÃ­a

**PerÃº21 (6 comandos):**
1. `scrape_peru21` - Portada general
2. `scrape_peru21D` - Deportes
3. `scrape_peru21G` - GastronomÃ­a
4. `scrape_peru21I` - InvestigaciÃ³n
5. `scrape_peru21L` - Lima

**CaracterÃ­sticas tÃ©cnicas:**
- Usan **Playwright** para navegaciÃ³n (JavaScript enabled)
- Detectan y extraen imÃ¡genes de alta calidad
- Manejan lazy-loading
- User-Agent realista
- Timeouts configurados (480s a 1800s segÃºn secciÃ³n)
- Transacciones DB para consistencia
- Logging detallado con emojis ğŸ“° ğŸ“„ âœ… âŒ
- Progress tracking en tiempo real

**Ejemplo: `scrape_elcomercio.py`**
```python
# Selectors especÃ­ficos del HTML de El Comercio
# Busca imÃ¡genes con clase "fs-wi__img"
# Extrae resoluciÃ³n de URLs tipo "...width=800&height=600..."
# Detecta enlaces a noticias en ".fs-wi__title a"
# Obtiene fecha de atributo "datetime" en <time>
```

#### **Vistas (lista de noticias):**
- `lista_noticias()` - El Comercio completo
- `politica()` - SecciÃ³n PolÃ­tica
- `economia()` - SecciÃ³n EconomÃ­a
- `mundo()`, `tecnologia()` - Igual
- `peru21()`, `peru21d()`, `peru21g()`, `peru21i()`, `peru21l()` - PerÃº21 por secciÃ³n

**Filtros disponibles en todas:**
- `?q=busqueda` - BÃºsqueda por tÃ­tulo/autor
- `?con_imagen=1` - Solo noticias con imagen
- `?fecha=hoy/ayer/semana/mes/rango` - Filtrar por fecha
- `?per_page=10/20/50` - PaginaciÃ³n
- Registra bÃºsquedas en tabla `Actividad`

#### **Vistas (ejecuciÃ³n de scraping):**
- Endpoints POST que lanzan tareas Celery asincrÃ³nicas
- Verifican rol premium/admin
- Registran actividad en tabla `Actividad`
- Devuelven `task_id` de Celery para monitoreo

#### **Tasks (Celery):**
- **`scrape_all_sections()`**: Ejecuta todos los scrapers en secuencia
- **`run_single_scrape(command_name)`**: Ejecuta un scraper especÃ­fico
  - Monitorea progreso en tiempo real
  - Actualiza estado Celery cada 3 segundos
  - Detecta outputs como "Se encontraron X noticias"
  - Calcula porcentaje completado
  - Timeout robusto con margen de seguridad
  - Retorna resumen: noticias procesadas, tiempo, Ã©xito/error

---

### **3ï¸âƒ£ App `analisis` - AnÃ¡lisis de Noticias con IA**

**PropÃ³sito:** Analizar contenido de noticias usando modelos de IA via OpenRouter.

#### **Modelos:**
- **`AnalisisNoticia`**
  - FK a `Noticia` + FK a `User` (Ãºnico: un anÃ¡lisis por usuario/noticia)
  - Campos de resultado:
    - `resumen` (TextField): Resumen ejecutivo
    - `sentimiento` (positivo/neutro/negativo)
    - `sentimiento_confianza` (0-1)
    - `categoria` (PolÃ­tica, EconomÃ­a, Tech, etc.)
    - `entidades` (JSON): Personas, organizaciones, lugares
    - `palabras_clave` (JSON array): 5 keywords ordenadas
  - Metadata: `estado` (pendiente/en_proceso/completado/error)
  - Tracking de tokens y costo estimado
  - Index en `(usuario, estado)`

#### **Vistas:**
- `lista_noticias_analisis()` - Muestra noticias analizables/no analizables
- `ver_resultado_analisis(analisis_id)` - Resultado completo de un anÃ¡lisis
- `mis_analisis()` - Panel con todos los anÃ¡lisis del usuario (paginado)

#### **API Endpoints:**
1. `POST /analisis/api/iniciar/<noticia_id>/`
   - Inicia anÃ¡lisis asincrÃ³nico
   - Verifica rol premium
   - Crea `AnalisisNoticia` con estado `en_proceso`
   - Lanza `analizar_noticia_task(noticia_id, user_id)`
   - Devuelve `task_id` para polling

2. `GET /analisis/api/estado/<task_id>/`
   - Devuelve estado de la tarea Celery
   - Progreso, status, resultado si ya acabÃ³

3. `GET /analisis/api/ultimo/<noticia_id>/`
   - Devuelve Ãºltimo anÃ¡lisis completado del usuario actual para esa noticia
   - Resumen, sentimiento, categorÃ­a, palabras clave

#### **Tasks (Celery):**
- **`analizar_noticia_task(noticia_id, user_id, max_retries=3)`**
  1. Obtiene la noticia y el AnalisisNoticia record
  2. Actualiza estado a `en_proceso`
  3. Scrapea contenido real de la URL (vuelve a parsear HTML con BeautifulSoup)
  4. EnvÃ­a a OpenRouter con prompt especÃ­fico
  5. Parsea JSON devuelto por la IA
  6. Guarda resultados en BD
  7. Reintentos automÃ¡ticos si hay rate limit (espera 60s)

**Prompt a IA (OpenRouter):**
```
Analiza la noticia y devuelve JSON con:
- resumen: 2-3 pÃ¡rrafos clave
- sentimiento: {label, confianza}
- categoria: Una de PolÃ­tica/EconomÃ­a/TecnologÃ­a/Deportes/GastronomÃ­a/InvestigaciÃ³n/Mundo/Lima
- entidades: {PERSON, ORG, LOC}
- palabras_clave: Array de 5 (ordenadas por relevancia)
```

**Selectores HTML para obtener contenido:**
- El Comercio: `div.story-contents__content` o `article`
- PerÃº21: `div.note__text` o `div.note-content` o fallback genÃ©rico

---

## ğŸ“Š Flujos Principales

### **Flujo 1: Scraping AutomÃ¡tico (cada 5 horas)**

```
Celery Beat (scheduler)
    â†“
beat ejecuta: scraping.tasks.scrape_all_sections
    â†“
Celery Worker
    â†“
run_single_scrape para cada una de las 10 secciones
    â†“
Management Command (ej: scrape_elcomercio)
    â†“
Playwright abre Chrome headless
    â†“
Navega a portada â†’ espera a .fs-wi
    â†“
Scrollea para lazy loading
    â†“
Extrae: tÃ­tulo, autor, fecha, imagen, enlace
    â†“
Inserta en BD (get_or_create evita duplicados)
    â†“
Devuelve resultado a Celery Beat
```

### **Flujo 2: Usuario solicita AnÃ¡lisis (Premium)**

```
Usuario hace POST a /analisis/api/iniciar/123/
    â†“
View verifica: usuario es premium/admin
    â†“
Crea AnalisisNoticia(estado='pendiente')
    â†“
Lanza analizar_noticia_task.delay(noticia_id, user_id)
    â†“
Devuelve task_id al frontend
    â†“
Frontend polling: GET /analisis/api/estado/task_id
    â†“
Celery Worker recibe la tarea
    â†“
Scrapea contenido HTML de la noticia
    â†“
Prepara prompt y llama OpenRouter API
    â†“
Parsea JSON (sentimiento, categorÃ­a, etc.)
    â†“
Guarda en AnalisisNoticia(estado='completado')
    â†“
Frontend detiene polling y muestra resultado
```

### **Flujo 3: Registro de Actividades**

```
Usuario hace acciÃ³n (login, vista, bÃºsqueda, compartir, scraping)
    â†“
View llama: registrar_login(), registrar_vista_noticia_actividad(), etc.
    â†“
Crea Actividad(usuario, tipo, descripcion, noticia?, datos_extra?)
    â†“
Se guarda en BD
    â†“
Aparece en profile como historial reciente
```

---

## ğŸ—ƒï¸ Esquema de BD

```
users (Django built-in)
â”œâ”€â”€ id, username, email, password, is_staff, is_superuser, etc.

profile
â”œâ”€â”€ id, user_id (FKâ†’users), role, fecha_registro

actividad
â”œâ”€â”€ id, usuario_id (FKâ†’users), tipo, descripcion, 
â”œâ”€â”€ noticia_id (FKâ†’noticia, nullable), datos_extra (JSON), fecha_actividad

noticia
â”œâ”€â”€ id, titulo, autor, fecha, imagen, enlace, origen, fecha_scraping

noticia_vistas
â”œâ”€â”€ id, usuario_id (FKâ†’users), noticia_id (FKâ†’noticia), fecha_vista
â”œâ”€â”€ unique_together: (usuario_id, noticia_id)

analisisnoticia
â”œâ”€â”€ id, noticia_id (FKâ†’noticia), usuario_id (FKâ†’users), 
â”œâ”€â”€ resumen, sentimiento, sentimiento_confianza, categoria,
â”œâ”€â”€ entidades (JSON), palabras_clave (JSON), task_id, estado, 
â”œâ”€â”€ tokens_usados, coste_estimado, creado_en, actualizado_en
â”œâ”€â”€ unique_together: (noticia_id, usuario_id)
```

---

## ğŸ” ConfiguraciÃ³n de Seguridad

**En `settings.py`:**
```python
DEBUG = os.getenv('DEBUG', 'True') == 'True'          # False en producciÃ³n
SECRET_KEY = os.getenv('SECRET_KEY', '...')          # De .env
ALLOWED_HOSTS = os.getenv('ALLOWED_HOSTS', '*').split(',')

# Middleware de seguridad
MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    ...
]

# Sesiones (1 hora)
SESSION_COOKIE_AGE = 3600
SESSION_EXPIRE_AT_BROWSER_CLOSE = True

# ProtecciÃ³n: LOGIN_URL redirige a /accounts/login/
@login_required  # Decorador en todas las vistas de noticias/anÃ¡lisis
```

**Control de acceso por rol:**
```python
def check_user_premium(user):
    """Solo usuarios premium/admin pueden iniciar anÃ¡lisis"""
    return user.is_staff or user.profile.role in ['premium', 'admin']

# En vistas:
if not check_user_premium(request.user):
    return JsonResponse({'error': 'permission_denied'}, status=403)
```

---

## ğŸ“¦ Dependencias Clave con Versiones

| LibrerÃ­a | VersiÃ³n | Uso |
|----------|---------|-----|
| Django | 5.2.6 | Framework web |
| Celery | 5.5.3 | Tareas asincrÃ³nicas |
| Playwright | 1.55.0 | Web scraping (navegador automated) |
| BeautifulSoup4 | 4.13.5 | Parsing HTML |
| openai | 2.8.1 | Cliente OpenRouter (compatible con OpenAI) |
| mysqlclient | 2.2.7 | Driver MySQL |
| redis | 6.4.0 | Cliente Python para Redis |
| requests | 2.32.5 | HTTP requests (para scraping adicional) |
| pydantic | 2.12.4 | ValidaciÃ³n de datos |
| python-dotenv | 1.2.1 | Cargar variables .env |
| PyJWT | 2.10.1 | Tokens JWT (si se usa API auth) |

---

## ğŸš€ CÃ³mo Ejecutar el Proyecto

### **Prerequisitos**
```bash
# 1. Python 3.9+ instalado
# 2. MySQL corriendo (por defecto localhost:3306)
# 3. Redis corriendo (por defecto localhost:6379)
```

### **InstalaciÃ³n y Setup**
```bash
# 1. Crear virtual environment
python -m venv env
source env/Scripts/activate  # Windows: env\Scripts\activate

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Crear archivo .env en raÃ­z del proyecto
cat > .env << EOF
DEBUG=True
SECRET_KEY=tu-secret-key-aqui
DB_ENGINE=django.db.backends.mysql
DB_NAME=elcomercio_db
DB_USER=root
DB_PASSWORD=
DB_HOST=localhost
DB_PORT=3306
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
OPENROUTER_API_KEY=tu-api-key-aqui
OPENROUTER_MODEL=openai/gpt-3.5-turbo
EOF

# 4. Migraciones BD
python manage.py makemigrations
python manage.py migrate

# 5. Crear super usuario
python manage.py createsuperuser

# 6. Recolectar archivos estÃ¡ticos (producciÃ³n)
python manage.py collectstatic --noinput
```

### **Ejecutar en Desarrollo**
```bash
# Terminal 1: Django dev server
python manage.py runserver

# Terminal 2: Celery Worker
celery -A web_scraping worker --loglevel=info --pool=solo

# Terminal 3: Celery Beat (tareas programadas)
python manage.py cleaned_beat
# O sin limpieza: celery -A web_scraping beat --loglevel=info

# Acceder a:
# - App: http://localhost:8000
# - Admin: http://localhost:8000/admin
```

### **Ejecutar Scraping Manual**
```bash
# Una secciÃ³n especÃ­fica
python manage.py scrape_elcomercio
python manage.py scrape_peru21

# Mediante API (requiere usuario premium en web):
# POST http://localhost:8000/scraping/scraping/lista
# AsincrÃ³nico: devuelve task_id de Celery
```

---

## ğŸ› Logging

```python
# settings.py configura loggers
LOGGING = {
    'loggers': {
        'analisis': {'level': 'DEBUG', 'handlers': ['file', 'console']},
        'scraping': {'level': 'DEBUG', 'handlers': ['file', 'console']},
    }
}

# Archivos generados:
# - debug.log (archivo local)
# - Salida de Celery por consola
```

---

## âœ… Funcionalidades Implementadas

- âœ… Scraping multi-secciÃ³n (El Comercio + PerÃº21)
- âœ… Almacenamiento en BD MySQL
- âœ… AutenticaciÃ³n y perfiles de usuario
- âœ… Roles (normal, premium, admin)
- âœ… AnÃ¡lisis de noticias con OpenRouter (IA)
- âœ… Tareas asincrÃ³nicas Celery + Redis
- âœ… Beat scheduler (scraping cada 5 horas)
- âœ… Tracking de actividades de usuarios
- âœ… Planes de suscripciÃ³n premium (simulado)
- âœ… BÃºsqueda y filtrado de noticias
- âœ… PaginaciÃ³n
- âœ… Notificaciones de progreso (WebSocket via Celery state)
- âœ… Admin Django personalizado

---

## ğŸš§ Ãreas de Mejora / TODO

1. **Seguridad:**
   - [ ] HTTPS en producciÃ³n
   - [ ] Rate limiting en endpoints de scraping
   - [ ] EncriptaciÃ³n de API keys en DB

2. **Performance:**
   - [ ] Cache Redis para listados de noticias
   - [ ] Ãndices DB optimizados
   - [ ] Lazy load en frontend (infinite scroll)

3. **Escalabilidad:**
   - [ ] DockerizaciÃ³n (Dockerfile + docker-compose.yml)
   - [ ] CI/CD pipeline (GitHub Actions)
   - [ ] Deployment a cloud (Heroku, AWS, Azure)

4. **Features nuevas:**
   - [ ] Notificaciones por email (destacadas)
   - [ ] CategorizaciÃ³n automÃ¡tica (sin IA)
   - [ ] Exportar anÃ¡lisis (PDF)
   - [ ] IntegraciÃ³n redes sociales (compartir)
   - [ ] Dashboard de analÃ­tica (para admin)
   - [ ] API REST pÃºblica (autenticada)
   - [ ] WebSocket real-time (progress scraping)

5. **Testing:**
   - [ ] Tests unitarios (pytest-django)
   - [ ] Tests de integraciÃ³n
   - [ ] Mocking de Playwright para CI

6. **DocumentaciÃ³n:**
   - [ ] API docs (DRF Swagger/OpenAPI)
   - [ ] Runbooks para ops
   - [ ] Architecture diagrams

---

## ğŸ“ Recomendaciones para Actualizar el README

El README actual estÃ¡ **desactualizado**. Se recomienda incluir:

### **Secciones propuestas:**
1. **DescripciÃ³n general:** Agregador de noticias + IA
2. **Stack tÃ©cnico:** Django 5.2.6, Celery, Playwright, OpenRouter
3. **Features:**
   - Scraping automÃ¡tico cada 5h
   - AnÃ¡lisis con IA (sentimiento, categorÃ­a, entidades)
   - GestiÃ³n de usuarios con roles premium
   - Tracking de actividades
4. **Estructura de carpetas:** Diagrama visual
5. **InstalaciÃ³n:** Paso a paso (.env, migraciones, deps)
6. **Uso:**
   - Dev: `python manage.py runserver` + `celery worker` + `beat`
   - Scraping: POST request o `manage.py scrape_*`
   - AnÃ¡lisis: API endpoint premium
7. **BD & Variables de entorno:** .env template
8. **Deploy:** Docker (nuevo), Heroku, etc.
9. **Troubleshooting:** Errores comunes
10. **Licencia y contacto**

---

## ğŸ“ ConclusiÃ³n

Este es un **proyecto full-stack profesional** que integra:
- Web scraping (Playwright)
- Processing async (Celery)
- IA generativa (OpenRouter)
- BD relacional (MySQL)
- AutenticaciÃ³n con roles
- Tracking de usuarios

EstÃ¡ **bien arquitecturado** pero tiene **oportunidades de escalabilidad** (Docker, cache, async WebSocket). El cÃ³digo es **limpio y documentado**, aunque le falta **cobertura de tests** y **docs para deploy**.

**RecomendaciÃ³n:** Actualizar README segÃºn la plantilla anterior y aÃ±adir Dockerfile + docker-compose.yml para facilitar setup local y deploy.

---

*AnÃ¡lisis realizado: 26/11/2025*
*Versiones confirmadas del stack tÃ©cnico.*
