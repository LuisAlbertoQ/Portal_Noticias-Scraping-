# ðŸ“° Portal de Noticias con Scraping e IA - Web Scraping Django

Un **agregador inteligente de noticias peruanas** desarrollado en Django que extrae automÃ¡ticamente contenido de **El Comercio** y **PerÃº21**, almacena en BD MySQL, y **analiza con IA** (OpenRouter) para extraer sentimiento, categorÃ­as y entidades. Incluye sistema de usuarios con roles, tareas asincrÃ³nicas con Celery y tracking de actividades.

## ðŸŽ¯ Â¿De quÃ© trata el proyecto?

Este proyecto es un **sistema completo de agregaciÃ³n, anÃ¡lisis y gestiÃ³n de noticias** que:

- ðŸ“° **Extrae automÃ¡ticamente** noticias de 2 portales peruanos (El Comercio + PerÃº21) cada 5 horas
- ðŸ” **Scrapea 10 secciones:** PolÃ­tica, EconomÃ­a, Mundo, TecnologÃ­a (El Comercio) + Deportes, GastronomÃ­a, InvestigaciÃ³n, Lima (PerÃº21)
- ðŸ¤– **Analiza con IA** (OpenRouter/GPT): resumen, sentimiento, categorÃ­a, entidades, palabras clave
- ðŸ‘¥ **Gestiona usuarios** con roles: normal (gratis), premium (anÃ¡lisis ilimitado), admin (acceso total)
- ðŸ“Š **Registra actividades** de todos los usuarios (login, vistas, bÃºsquedas, anÃ¡lisis, scraping)
- âš¡ **Ejecuta tareas asincrÃ³nicas** con Celery + Redis (sin bloquear la app)
- ðŸŽ¨ **Interfaz moderna y responsive** con bÃºsqueda, filtros avanzados y paginaciÃ³n

---

## ðŸ› ï¸ TecnologÃ­as Utilizadas

### Backend & Scraping
- **Django 5.2.6** - Framework web principal
- **Python 3.9+** - Lenguaje de programaciÃ³n
- **Celery 5.5.3** - Tareas asincrÃ³nicas
- **Redis 6.4.0** - Broker de mensajes
- **Playwright 1.55.0** - AutomatizaciÃ³n de navegadores (JavaScript enabled)
- **BeautifulSoup4 4.13.5** - Parsing y extracciÃ³n de datos HTML
- **MySQL (mysqlclient 2.2.7)** - Base de datos relacional

### IA & APIs
- **OpenRouter (openai 2.8.1)** - Cliente para anÃ¡lisis con modelos LLM
- **Pydantic 2.12.4** - ValidaciÃ³n de datos
- **httpx 0.28.1** - HTTP client asincrÃ³nico

### Frontend
- **HTML5/CSS3** - Interfaz de usuario responsive
- **JavaScript (vanilla)** - Interactividad y polling de tareas Celery
- **Bootstrap** - Estilos base
- **Font Awesome** - IconografÃ­a

---

## ðŸ“‹ Requisitos del Sistema

### Requisitos Hardware
- **Procesador:** 2GHz dual-core
- **RAM:** 4GB mÃ­nimo (8GB recomendado para Celery + BD)
- **Disco:** 500MB libre

### Requisitos Software
- **Python 3.9+** (3.13 recomendado)
- **MySQL 5.7+** o MariaDB 10.3+
- **Redis 6.0+** (para Celery broker)
- **pip** y **Git**

---

## ðŸš€ Manual de Despliegue (EjecuciÃ³n Local)

### Prerequisitos: Instalar Dependencias del Sistema

#### **Windows**

**1. Instalar MySQL:**
- Descargar: https://dev.mysql.com/downloads/mysql/
- Ejecutar installer (Next â†’ Next â†’ Finish)
- Anotar usuario/password (por defecto: root/sin password)

**2. Instalar Redis (OpciÃ³n A - Windows Subsystem for Linux 2):**
```powershell
# Abrir PowerShell como Admin
wsl --install
# Reiniciar y ejecutar en WSL:
sudo apt-get update && sudo apt-get install redis-server
```

**OpciÃ³n B - Usar Docker:**
```powershell
# Si tienes Docker instalado
docker run -d -p 6379:6379 --name redis redis:latest
```

#### **macOS**

```bash
# Instalar MySQL
brew install mysql
brew services start mysql

# Instalar Redis
brew install redis
brew services start redis
```

#### **Linux (Ubuntu/Debian)**

```bash
sudo apt-get update
sudo apt-get install mysql-server redis-server

sudo systemctl start mysql
sudo systemctl start redis-server
```

---

### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/LuisAlbertoQ/Portal_Noticias-Scraping-.git
cd Portal_Noticias-Scraping-
```

---

### Paso 2: Crear Entorno Virtual

```bash
# Windows (PowerShell)
python -m venv env
.\env\Scripts\Activate.ps1

# Windows (CMD)
python -m venv env
.\env\Scripts\activate.bat

# Linux/Mac
python -m venv env
source env/bin/activate
```

---

### Paso 3: Instalar Dependencias Python

```bash
pip install -r requirements.txt
```

---

### Paso 4: Instalar Navegadores de Playwright

```bash
playwright install chromium
```

---

### Paso 5: Configurar Variables de Entorno

**Crear archivo `.env` en la raÃ­z del proyecto:**

```bash
# Windows (PowerShell)
@"
# ConfiguraciÃ³n General
DEBUG=True
SECRET_KEY=django-insecure-tu-clave-secreta-aqui-cambia-en-produccion
ALLOWED_HOSTS=127.0.0.1,localhost

# Base de Datos MySQL
DB_ENGINE=django.db.backends.mysql
DB_NAME=elcomercio_db
DB_USER=root
DB_PASSWORD=
DB_HOST=127.0.0.1
DB_PORT=3306

# Celery + Redis
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
CELERY_TIMEZONE=America/Lima

# OpenRouter (IA) - ObtÃ©n tu key en https://openrouter.ai
OPENROUTER_API_KEY=tu_api_key_aqui
OPENROUTER_MODEL=openai/gpt-3.5-turbo
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Sesiones
SESSION_COOKIE_AGE=3600
SESSION_SAVE_EVERY_REQUEST=True
SESSION_EXPIRE_AT_BROWSER_CLOSE=True
"@ | Out-File -Encoding UTF8 .env
```

```bash
# Linux/Mac
cat > .env << 'EOF'
# ConfiguraciÃ³n General
DEBUG=True
SECRET_KEY=django-insecure-tu-clave-secreta-aqui-cambia-en-produccion
ALLOWED_HOSTS=127.0.0.1,localhost

# Base de Datos MySQL
DB_ENGINE=django.db.backends.mysql
DB_NAME=elcomercio_db
DB_USER=root
DB_PASSWORD=
DB_HOST=127.0.0.1
DB_PORT=3306

# Celery + Redis
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
CELERY_TIMEZONE=America/Lima

# OpenRouter (IA) - ObtÃ©n tu key en https://openrouter.ai
OPENROUTER_API_KEY=tu_api_key_aqui
OPENROUTER_MODEL=openai/gpt-3.5-turbo
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Sesiones
SESSION_COOKIE_AGE=3600
SESSION_SAVE_EVERY_REQUEST=True
SESSION_EXPIRE_AT_BROWSER_CLOSE=True
EOF
```

---

### Paso 6: Crear Base de Datos MySQL

```bash
# OpciÃ³n 1: Con MySQL CLI (interactivo)
mysql -u root -p
# Luego ejecutar en MySQL:
# CREATE DATABASE elcomercio_db;
# EXIT;

# OpciÃ³n 2: Directamente (sin contraseÃ±a)
mysql -u root -e "CREATE DATABASE elcomercio_db;"
```

---

### Paso 7: Ejecutar Migraciones de BD

```bash
python manage.py migrate
```

---

### Paso 8: Crear Superusuario (Admin)

```bash
python manage.py createsuperuser
```

Responde las preguntas interactivas:
- **Username:** admin (o tu nombre)
- **Email:** admin@example.com
- **Password:** (elige una contraseÃ±a)

---

### Paso 9: Recolectar Archivos EstÃ¡ticos

```bash
python manage.py collectstatic --noinput
```

---

## âš¡ Manual de EjecuciÃ³n Local

### OpciÃ³n A: EjecuciÃ³n Simple (SIN Tareas AsincrÃ³nicas)

**Terminal 1: Django Development Server**

```bash
python manage.py runserver
```

âœ… Accede a: **http://127.0.0.1:8000**

âš ï¸ **Limitaciones:** El scraping automÃ¡tico y anÃ¡lisis de IA no funcionarÃ¡n sin Celery/Redis.

---

### OpciÃ³n B: EjecuciÃ³n Completa (RECOMENDADO - Con Celery + Redis)

**Requisito previo:** Verificar que Redis estÃ¡ corriendo

```bash
# Verificar Redis
redis-cli ping
# Debe responder: PONG
```

**Terminal 1: Django Development Server**

```bash
python manage.py runserver
```

**Terminal 2: Celery Worker** (ejecuta tareas asincrÃ³nicas)

```bash
celery -A web_scraping worker --loglevel=info --pool=solo
```

**Terminal 3: Celery Beat** (ejecuta scraping cada 5 horas)

```bash
python manage.py cleaned_beat
```

O sin limpieza automÃ¡tica:

```bash
celery -A web_scraping beat --loglevel=info
```

**Terminal 4: Redis Server** (si no estÃ¡ ejecutÃ¡ndose como servicio)

```bash
# En WSL/Linux/Mac
redis-server

# En Windows (si usaste Docker)
docker run -d -p 6379:6379 redis
```

---

## ðŸŒ Acceso a la AplicaciÃ³n

Una vez ejecutando, accede a:

| URL | DescripciÃ³n | Requiere Login |
|-----|------------|----------------|
| http://127.0.0.1:8000 | PÃ¡gina principal / Bienvenida | âŒ |
| http://127.0.0.1:8000/accounts/register | Registro de usuarios | âŒ |
| http://127.0.0.1:8000/accounts/login | Iniciar sesiÃ³n | âŒ |
| http://127.0.0.1:8000/accounts/profile | Perfil de usuario | âœ… |
| http://127.0.0.1:8000/scraping/elcomercio | Noticias El Comercio | âœ… |
| http://127.0.0.1:8000/scraping/peru21 | Noticias PerÃº21 | âœ… |
| http://127.0.0.1:8000/analisis | AnÃ¡lisis de noticias | âœ… Premium/Admin |
| http://127.0.0.1:8000/admin | Panel de administraciÃ³n | âœ… Admin |

---

## ðŸ“– Manual de Usuario

### 1. Registro e Inicio de SesiÃ³n

**Crear una cuenta nueva:**
1. Ir a `/accounts/register`
2. Llenar: Username, Email, ContraseÃ±a
3. Hacer clic en "Registrarse"
4. SerÃ¡s redirigido automÃ¡ticamente al listado de noticias

**Iniciar sesiÃ³n:**
1. Ir a `/accounts/login`
2. Ingresar Username/Email y ContraseÃ±a
3. Hacer clic en "Iniciar SesiÃ³n"

---

### 2. Visualizar Noticias

**Secciones disponibles:**

**El Comercio:**
- `/scraping/elcomercio` - Todas las noticias
- `/scraping/elcomercio/politica` - SecciÃ³n PolÃ­tica
- `/scraping/elcomercio/economia` - SecciÃ³n EconomÃ­a
- `/scraping/elcomercio/mundo` - SecciÃ³n Mundo
- `/scraping/elcomercio/tecnologia` - SecciÃ³n TecnologÃ­a

**PerÃº21:**
- `/scraping/peru21` - Todas las noticias
- `/scraping/peru21/deportes` - SecciÃ³n Deportes
- `/scraping/peru21/gastronomia` - SecciÃ³n GastronomÃ­a
- `/scraping/peru21/investigacion` - SecciÃ³n InvestigaciÃ³n
- `/scraping/peru21/lima` - SecciÃ³n Lima

---

### 3. Filtrar y Buscar Noticias

En cualquier pÃ¡gina de noticias, tienes:

**ðŸ” BÃºsqueda:**
- Ingresa tÃ©rmino en la barra de bÃºsqueda
- Busca por **tÃ­tulo** o **autor**

**ðŸ“… Filtrar por fecha:**
- **Hoy** - Noticias de hoy
- **Ayer** - Noticias de ayer
- **Ãšltima semana** - Ãšltimos 7 dÃ­as
- **Ãšltimo mes** - Ãšltimos 30 dÃ­as
- **Rango personalizado** - Selecciona fechas especÃ­ficas

**ðŸ–¼ï¸ Filtrar por imagen:**
- Marca "Solo noticias con imagen"

**ðŸ“Š PaginaciÃ³n:**
- Selecciona 10, 20 o 50 noticias por pÃ¡gina

---

### 4. Analizar Noticias con IA (Premium)

Para acceder a esta funciÃ³n, necesitas ser **usuario Premium**.

**Actualizar a Premium:**
1. Ir a tu Perfil (`/accounts/profile`)
2. Hacer clic en "Planes y SuscripciÃ³n"
3. Hacer clic en "Actualizar a Premium"
4. Confirmar (simulado - en producciÃ³n usarÃ­as Stripe)

**Analizar una noticia:**
1. Ir a `/analisis` (solo disponible para premium)
2. Seleccionar una noticia que deseas analizar
3. Hacer clic en botÃ³n "Analizar"
4. Esperar a que Celery procese (puede tomar 5-30s)
5. Ver resultados: **Resumen, Sentimiento, CategorÃ­a, Entidades, Palabras Clave**

**Ver mis anÃ¡lisis:**
1. En tu Perfil (`/accounts/profile`), secciÃ³n "Mis AnÃ¡lisis Recientes"
2. O ir directamente a `/analisis/mis-analisis/`

---

### 5. Ejecutar Scraping

#### **VÃ­a Web UI (Recomendado):**
1. En cualquier pÃ¡gina de noticias (El Comercio o PerÃº21)
2. Hacer clic en botÃ³n "Ejecutar Scraping" (solo premium/admin)
3. Se abrirÃ¡ modal con progreso en tiempo real
4. Esperar a que termine (5-15 minutos segÃºn cantidad)

#### **VÃ­a LÃ­nea de Comandos (Manual):**

```bash
# Todas las secciones a la vez
python manage.py scrape_all_sections

# El Comercio (todas las secciones)
python manage.py scrape_elcomercio
python manage.py scrape_economia
python manage.py scrape_elcomercio_pol
python manage.py scrape_mundo
python manage.py scrape_tecnologia

# PerÃº21 (todas las secciones)
python manage.py scrape_peru21
python manage.py scrape_peru21D  # Deportes
python manage.py scrape_peru21G  # GastronomÃ­a
python manage.py scrape_peru21I  # InvestigaciÃ³n
python manage.py scrape_peru21L  # Lima
```

---

### 6. Ver Perfil y Actividades

En tu Perfil (`/accounts/profile`), verÃ¡s:

- **ðŸ“Š EstadÃ­sticas Personales:**
  - DÃ­as que llevas activo
  - Noticias vistas
  - AnÃ¡lisis realizados
  - Rol actual (Normal/Premium/Admin)

- **ðŸ“° Noticias Vistas Recientemente:** Ãšltimas 5 noticias que abriste

- **ðŸ¤– AnÃ¡lisis Recientes:** Ãšltimos 5 anÃ¡lisis de IA realizados

- **ðŸ“ Actividades Recientes:** Historial de login, bÃºsquedas, vistas, compartidas, etc.

---

### 7. Compartir Noticias

En cada noticia, hay botones para compartir (simulado en frontend):
- **Facebook**
- **Twitter/X**
- **WhatsApp**
- **Email**

Cada compartir se registra en tu historial de actividades.

---

### 8. Panel de AdministraciÃ³n

**Solo para Admins:**

Accede a `/admin/` con credenciales de superusuario.

Desde aquÃ­ puedes:

- **Gestionar Usuarios:** Ver, crear, editar roles
- **Ver Perfiles:** InformaciÃ³n de cada usuario
- **Gestionar Noticias:** Crear, editar, eliminar noticias
- **Ver AnÃ¡lisis:** Historial de anÃ¡lisis realizados
- **Ver Actividades:** AuditorÃ­a completa de quÃ© hizo cada usuario
- **Gestionar Grupos:** (Django built-in)

---

## ðŸ”§ Comandos Ãštiles

### Desarrollo

```bash
# Iniciar servidor de desarrollo
python manage.py runserver

# Crear migraciones (despuÃ©s de cambiar models.py)
python manage.py makemigrations

# Aplicar migraciones
python manage.py migrate

# Acceder al shell de Django
python manage.py shell

# Ver estado de migraciones
python manage.py showmigrations

# Resetear base de datos COMPLETA (âš ï¸ borra todo)
python manage.py flush
```

### Scraping Manual

```bash
# Todas las secciones
python manage.py scrape_all_sections

# El Comercio
python manage.py scrape_elcomercio
python manage.py scrape_economia
python manage.py scrape_elcomercio_pol
python manage.py scrape_mundo
python manage.py scrape_tecnologia

# PerÃº21
python manage.py scrape_peru21
python manage.py scrape_peru21D
python manage.py scrape_peru21G
python manage.py scrape_peru21I
python manage.py scrape_peru21L
```

### Celery

```bash
# Worker (ejecuta tareas)
celery -A web_scraping worker --loglevel=info --pool=solo

# Beat (ejecuta tareas programadas)
celery -A web_scraping beat --loglevel=info

# Con limpieza automÃ¡tica de schedule
python manage.py cleaned_beat

# Monitorear tasks (en otra terminal)
celery -A web_scraping events
```

---

## ðŸ“Š Estructura del Proyecto

```
Portal_Noticias-Scraping-/
â”œâ”€â”€ accounts/                          # GestiÃ³n de usuarios
â”‚   â”œâ”€â”€ models.py                     # Profile, Actividad
â”‚   â”œâ”€â”€ views.py                      # Auth (login, register, profile, premium)
â”‚   â”œâ”€â”€ forms.py                      # RegistroForm
â”‚   â”œâ”€â”€ admin.py                      # Admin personalizado
â”‚   â””â”€â”€ urls.py                       # Rutas
â”‚
â”œâ”€â”€ scraping/                          # Web scraping
â”‚   â”œâ”€â”€ models.py                     # Noticia, NoticiasVistas
â”‚   â”œâ”€â”€ views.py                      # Listados por secciÃ³n
â”‚   â”œâ”€â”€ tasks.py                      # Celery tasks
â”‚   â”œâ”€â”€ urls.py                       # Rutas
â”‚   â”œâ”€â”€ management/commands/          # Django commands
â”‚   â”‚   â”œâ”€â”€ scrape_elcomercio.py
â”‚   â”‚   â”œâ”€â”€ scrape_economia.py
â”‚   â”‚   â”œâ”€â”€ scrape_elcomercio_pol.py
â”‚   â”‚   â”œâ”€â”€ scrape_mundo.py
â”‚   â”‚   â”œâ”€â”€ scrape_tecnologia.py
â”‚   â”‚   â”œâ”€â”€ scrape_peru21.py
â”‚   â”‚   â”œâ”€â”€ scrape_peru21D.py
â”‚   â”‚   â”œâ”€â”€ scrape_peru21G.py
â”‚   â”‚   â”œâ”€â”€ scrape_peru21I.py
â”‚   â”‚   â”œâ”€â”€ scrape_peru21L.py
â”‚   â”‚   â””â”€â”€ cleaned_beat.py
â”‚   â””â”€â”€ templates/                    # HTML templates
â”‚
â”œâ”€â”€ analisis/                          # AnÃ¡lisis con IA
â”‚   â”œâ”€â”€ models.py                     # AnalisisNoticia
â”‚   â”œâ”€â”€ views.py                      # API endpoints
â”‚   â”œâ”€â”€ tasks.py                      # analizar_noticia_task
â”‚   â”œâ”€â”€ urls.py                       # Rutas
â”‚   â””â”€â”€ admin.py                      # Admin
â”‚
â”œâ”€â”€ web_scraping/                      # ConfiguraciÃ³n Django
â”‚   â”œâ”€â”€ settings.py                   # ConfiguraciÃ³n global
â”‚   â”œâ”€â”€ celery.py                     # ConfiguraciÃ³n Celery
â”‚   â”œâ”€â”€ urls.py                       # URLs globales
â”‚   â””â”€â”€ wsgi.py                       # WSGI app
â”‚
â”œâ”€â”€ templates/                         # Plantillas globales
â”‚   â”œâ”€â”€ base.html                     # Base template
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ static/                            # CSS, JS, imÃ¡genes
â”‚   â”œâ”€â”€ css/
â”‚   â””â”€â”€ js/
â”‚
â”œâ”€â”€ manage.py                          # Script de gestiÃ³n Django
â”œâ”€â”€ requirements.txt                   # Dependencias
â”œâ”€â”€ ANALISIS_PROYECTO.md               # AnÃ¡lisis tÃ©cnico completo
â””â”€â”€ README.md                          # Este archivo
```

---

## ðŸ› SoluciÃ³n de Problemas

### Error: "No such table: accounts_profile"

**SoluciÃ³n:**
```bash
python manage.py migrate
```

### Error: "Connection refused" en Redis

**SoluciÃ³n:**
```bash
# Verificar Redis estÃ¡ corriendo
redis-cli ping
# Si no: iniciar Redis
redis-server

# En Windows (si usas Docker)
docker run -d -p 6379:6379 redis
```

### Error: "Database doesn't exist"

**SoluciÃ³n:**
```bash
# Crear BD
mysql -u root -e "CREATE DATABASE elcomercio_db;"

# O manualmente:
mysql -u root -p
# CREATE DATABASE elcomercio_db;
```

### Error: "No module named 'django'"

**SoluciÃ³n:**
```bash
# Verificar que el venv estÃ¡ activado
# Luego reinstalar:
pip install -r requirements.txt
```

### Error: "Playwright: browser not found"

**SoluciÃ³n:**
```bash
playwright install chromium
```

### El scraping se queda en "Procesando..."

**Posibles causas:**
1. Celery worker no estÃ¡ corriendo (Terminal 2)
2. Redis no estÃ¡ disponible
3. Las URLs de los portales cambiaron (selectors rotos)

**SoluciÃ³n:**
```bash
# Ver logs de Celery worker para debug
celery -A web_scraping worker --loglevel=debug --pool=solo
```

---

## ðŸ“Š CaracterÃ­sticas Implementadas

- âœ… **Scraping inteligente** de 2 portales peruanos (10 secciones)
- âœ… **Base de datos MySQL** con relaciones optimizadas
- âœ… **AutenticaciÃ³n** con roles (normal, premium, admin)
- âœ… **AnÃ¡lisis con IA** (OpenRouter): sentimiento, categorÃ­as, entidades
- âœ… **Tareas asincrÃ³nicas** (Celery + Redis)
- âœ… **Scraping automÃ¡tico** cada 5 horas (Celery Beat)
- âœ… **Tracking de actividades** de usuarios
- âœ… **Interfaz responsive** con filtros avanzados
- âœ… **BÃºsqueda** por tÃ­tulo y autor
- âœ… **PaginaciÃ³n** configurable
- âœ… **Panel admin** personalizado
- âœ… **Manejo robusto de errores** y timeouts

---

## ðŸš§ Ãreas de Mejora

- [ ] DockerizaciÃ³n (Dockerfile + docker-compose.yml)
- [ ] Tests unitarios (pytest-django)
- [ ] CI/CD (GitHub Actions)
- [ ] WebSocket real-time (en lugar de polling)
- [ ] Exportar anÃ¡lisis a PDF
- [ ] Notificaciones por email
- [ ] Dashboard de analÃ­tica (admin)
- [ ] API REST pÃºblica (OAuth2)
- [ ] IntegraciÃ³n redes sociales
- [ ] Almacenamiento S3 para imÃ¡genes

---

## ðŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

---

## ðŸ‘¨â€ðŸ’» Autor

- **Luis Alberto Q** - [@LuisAlbertoQ](https://github.com/LuisAlbertoQ)

---

## ðŸ“ž Soporte

Si tienes problemas o preguntas:
- Abre un [Issue en GitHub](https://github.com/LuisAlbertoQ/Portal_Noticias-Scraping-/issues)
- Revisa [ANALISIS_PROYECTO.md](ANALISIS_PROYECTO.md) para detalles tÃ©cnicos

---

**Â¡Disfruta analizando noticias con IA! ðŸ“°âœ¨**
